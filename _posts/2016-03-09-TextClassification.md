---
layout:     post
title:      "文本分类方法综述"
subtitle:   " \"Method Introduction!\""
date:       2016-03-09
author:     "fibears"
header-img: "img/in-post/header/post-bg-text.png"
tags:
    - 文本挖掘
    - Article
---

文本分类是模式识别与自然语言处理密切结合的研究课题。文本分类是在预定义的分类体系下，根据文本的特征（内容或属性），将给定文本与一个或多个类别相关联的过程。也就是说文本分类的最终目的是要找到一个有效的映射函数，准确地将样本映射到对应的分类体系中，这个映射函数就是我们通常所说的分类器。

根据分类知识获取方法的不同，文本自动分类系统大致可分为两种类型：基于知识工程的分类系统和基于统计机器学习的分类系统。前者以知识工程的方法为主，根据领域专家对给定文本集合的分类经验，人工提取出一组逻辑规则，作为计算机文本分类的依据；而后者主要令训练样本进行特征选择和分类器参数训练，根据选择的特征对待分类的输入样本进行标准化，然后输入到分类器模型中进行类别判断，最终得到输入样本的类别。

## 向量空间模型简介
一个文本通常表现为一个由文字和标点符号组成的字符串，由字或字符组成词，由词组成短语，进而形成句、段、节、章、篇的结构。目前文本表示通常从采用向量空间模型，它已经成为自然语言处理中常用的模型。

以下是向量空间模型中几个最基本的概念：

- 文档：文章中具有一定规模地片段
- 特征项：特征项是向量空间模型中最小的不可分的语言单元
- 项的权重：对于含有n个特征项的文档，每一个特征项都依据一定的原则被赋予一个权重，表示它们在文档中的重要程度。

> _***向量空间模型：***_
> > 给定一个符合以下两条约定的文档D：
> > 	1. 各个特征项之间互异
> > 	2. 各个特征想之间无先后顺序关系
> > 满足这两个约定条件下，可以把特征项看出一个n维坐标系，而权重表示相应的坐标值，因此一个文本就表示为n维空间中的一个向量。
> 
> _***向量的相似性度量：***_
> > 任意两个文档直接的相似系数指两个文档内容的相似度，可以借助n维空间中两个向量之间的某种距离来表示文档间的相似系数，常用的方法是使用向量之间的内积或两个向量夹角的余弦值来计算。
>
> _***向量空间模型的步骤：***_
> 首先根据训练样本集生成文本表示所需要的特征项序列D；依据文本特征项序列，对训练文本集和测试集中的各个文档进行权重赋值、规范化等处理，将其转化为机器学习算法所需的特征向量。

## 文本特征提取方法

由于在传统的向量空间模型中，特征项之间互异的假设条件在实际环境中往往很难满足，而且该方法所构建的特征矩阵通常是超高维的，这给后续的研究带来非常大的麻烦。因此我们需要简化特征矩阵，相应的工作即为文本特征提取。特征提取技术是文本挖掘模型的基础和核心，从广义上来讲，特征提取技术可以分为选择和变换两种思路，前者主要通过评估函数筛选出部分特征项，比如DF算法、信息增益法和互信息等方法构建衡量文本特征项重要程度的函数，从而决定对该特征项的取舍。特征选择技术虽然构造简单、适用范围广，但是该方法主观性较强，且需要充足的先验信息和特征相互独立的假设，这在实际文本中很难满足。而后者则是将原始特征项降维到低维空间中，比如基于主成分分析法或者主题模型来提取文本特征项，将词项特征空间转换为成分或主题表示。

### 基于特征选择思想的方法

- DF(Document frequency)法：DF是指出现某个特征项的文档的频率，该方法从训练语料中统计出包含某个特征的文档的频率，然后根据设定的阈值筛选特征项。
- 信息增益法：依据特征项为整个分类所能提供的信息量多少来衡量该特征项的重要程度，信息量的多少由熵来衡量，信息增益即不考虑任何特征时文档的熵和考虑该特征后文档的熵的差值。
- CHI统计量法：利用卡方统计量来衡量特征项与类别直接的关联程度，特征对于某类的卡方统计值越高，它与该类之间的相关性越大，携带的信息也较多。
- 互信息法：其基本思想为互信息越大，特征和类别共现的程度越大。互信息\\(I(t_i,C_j) = log \frac{P(t_i, C_j)}{P(t_i)P(C_j)}\\)

### 特征权重计算方法
挑选出特征项后，我们需要计算每个文档中特征项的重要程度，即特征权重值。权重计算的一般方法是利用文本的统计信息（词频），给特征项赋予一定的权重。

_***常用的特征权重计算方法有：***_

符号说明：\\(w_{ij}表示特征项t_i在文档D_j中的权重；tf_{ij}表示特征项t_i在文档D_j中的频数\\)；\\(n_i表示训练集中出现特征项t_i的文档数；N表示训练集中总文档数；M表示特征项个数\\)；\\(nt_i表示特征项t_i在训练语料中出现的次数\\)。

- 布尔权重法：文本中出现特征项时，那么该文本向量的对应分量为1，否则为0；
- 绝对词频(Term Freq):使用特征项在文本中出现的频率表示文本；
- 倒排文档频度(Inverse Document Freq): \\(w_{ij} = log \frac{N}{n_i}\\)
- TF-IDF：权重与特征项在文档中出现的频率成正比，与整个语料中出现该特征项的文档数成反比。\\(w_{ij} = tf_{ij}*log \frac{N}{n_i}\\)
- TFC：对文本长度进行归一化处理后的TF-IDF
- ITC：在TFC基础上，用\\(tf_{ij}\\)的对数值代替\\(tf_{ij}\\)值

_*备注：权重计算方法和特征提取方法存在类似的问题，即缺少理论上的推导和验证，因此表现出来的非一般性结果无法得到合理的解释。*_

## 分类器算法

由于文本分类本身也是一个分类问题，因此只要我们把特征项提取出来并赋予一定的权重后即可利用常用的分类器模型进行分类研究。常用的分类器方法主要有：朴素贝叶斯分类法、支持向量机模型、KNN、神经网络模型、决策树分类模型等。

- SVM思想：在向量空间中找到一个决策平面，该平面能“最好”地分割两个分类中的数据点，支持向量机模型就是要在训练集中找到具有最大分类间隔的决策平面。
- 决策树模型思想：决策树是一个树状模型，树的根节点表示整个数据空间，每个分节点处利用信息增益等方法决定某个变量的划分标准，通过该节点将数据集合空间分割成两个或多个类别，每个叶子节点表示最终分类的类别。构建决策树时，首先利用训练数据生成决策树，然后通过测试数据进行修剪，当模型构建完毕时，每一条从根节点到叶子节点的路径表示一条决策规则。
- 基于投票的分类方法：k个分类器预测结果的有效组合优于某个分类器的预测结果，主要分为Bagging和Boosting。
- Bagging算法：利用Bootstrap思想，每次从样本总体中抽取N个文档构建新的训练集，然后利用该训练集拟合分类器模型，重复K次试验，得到K个分类器模型。对于新文档D，利用这K个分类器分别进行预测，取得票数最多的那个类别作为最终分类结果。
- Boosting算法：该算法类似于Bagging算法，唯一不同的地方在于，Boosting每次构建新模型时所利用的数据是完整的样本数据集，而不是利用Bootsrap方法抽样得到的。

## 分类性能评测

提到分类性能评测，必然少不了混淆矩阵(Confusion Matrix):

$$
\begin{array}{c|c|c}
\hline
 & 预测YES & 预测NO \\
\hline
真实YES & TP & FN \\
真实NO & FP & TN \\
\hline
\end{array}
$$

- 准确率：\\(\frac{TP+TN}{TP+TN+FP+FN}\\)
- 真阳率(TPR)：\\(\frac{TP}{TP+FN}\\)
- 假阳率(FPR):\\(\frac{FN}{TP+FN}\\)
- 真阴率(TNR):\\(\frac{TN}{TN+FP}\\)
- 假阴率(FNR):\\(\frac{FP}{TN+FP}\\)
- 正确率(精度)：预测为正类的样本中真正为正类的比例\\(\frac{TP}{TP+FP}\\);[你的正类预测结果中有多少是对的]
- 召回率：真正为正类的样本中被正确预测样本的比例\\(\frac{TP}{TP+FN}\\);[正类结果中你成功预测了多少]
- F1测度值：正确率和召回率的调和平均数

> 调和平均数（harmonic mean）又称倒数平均数，是总体各统计变量倒数的算术平均数的倒数。[调和平均数可以用在相同距离但速度不同时，平均速度的计算]












